\documentclass[10pt]{article}  
\usepackage{graphicx}
\usepackage{geometry}   %设置页边距的宏包
\usepackage{comment}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{verbatim}
\usepackage{microtype}
\usepackage{kpfonts}
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{array}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\Qb}{\mathbf{Q}}
\newcommand{\Rb}{\mathbf{R}}
\newcommand{\Nb}{\mathbf{N}}
\newcommand{\Fb}{\mathbf{F}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Lap}{\mathcal{L}}
\newcommand{\Zplus}{\mathbf{Z}^+}
\geometry{left=1cm,right=1cm,top=1cm,bottom=1.5cm}  %设置 上、左、下、右 页边距

\begin{document}  
	\begin{multicols}{2}
		\begin{enumerate}
			\item Moment Generating Function\\
			$$M_X(t) = E(e^{tx})$$
			continuous: $$M_X(t) = \int_{-\infty}^{\infty}e^{tx}P_x(x)dx$$
			discrete: $$M_X(t) = \sum e^{tx}P_x(x)$$
			$$\frac{dM_x(t)}{dt} = E(\frac{de^{tx}}{dt}) = E(xe^{tx})\Rightarrow M^{(n)}_x(t) = E(x^ne^{tx})$$
			Properties:
			\begin{enumerate}
				\item $M_x(t) = E(\sum_{n=0}^{\infty}\frac{x^nt^n}{n!})$ according to Taylor Series\\
				$M'_x(t) = E(0+x+x^2t + \frac{x^3t^2}{2!} + \dots) = E(\sum_{n=0}^{\infty}\frac{x^{n+1}t^n}{n!})$
				$\Rightarrow M^{(k)}_x(t) = E(\sum_{n=0}^{\infty}\frac{x^{n+k}t^n}{n!})$
				\item $M_x^{(n)}(0) = E(X^n) \\ \Rightarrow VAR[x]=E[x^2] - E^2[x] = M_x''(0) - [M_x'(0)]^2 $
				\item $M_x(t) = M_y(t) \Rightarrow X Y$ has same distribution
				\item $M_x(0) = 1$
				\item $M_{ax + b}(t) = M_x(at)e^{bt}$   Proof:\\ $M_{ax+b}(t) = E(e^{t(ax+b)}) = E(e^{axt}e^{bt}) = E(e^{axt})e^{bt} = M_x(at)e^{bt}$
				\item $M_{X+Y}(bt) = M_X(t)M_Y(t)$ ($X,Y$ independent)
			\end{enumerate}
		
			\item Gama Function\\
			$$\Gamma(n) = \int_{0}^{\infty}u^{n-1}e^{-u}du = (n-1)!$$
			Proof:\\
			1. $\int_{0}^{\infty}e^{-x}dx = -e^{-x}|_0^{\infty} = 1$\\
			2. $\Gamma(n) = -\int_{0}^{\infty}x^{n-1}de^{-x}\\
			= - (x^{n-1}e^{-x}|_0^{\infty} - \int_{0}^{\infty}e^{-x}(n-1)x^{n-1}dx)\\
			= 0 + \int_{0}^{\infty}e^{-x}(n-1)x^{n-2}dx\\
			= (n-1)\int_{0}^{\infty}e^{-x}x^{n-2}dx\\
			= (n-1)\Gamma(n-1)$\\
			Use Induction to prove the formula\\
			Formula: $\int_{0}^{1}x^{\alpha-1}(1-x)^{\beta-1}dx = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}$
			
			\item Basic probability property
			\begin{enumerate}
				\item Basic Properties\\
				1. $P(E) = \frac{n(E)}{n(S)}, \in[0,1]$\\
				2. $P(\phi) = 0$\\
				3. $A\subseteq B, P(A) \le P(B)$\\
				4. $P(A\cup B) = P(A) + P(B) - P(A\cap B)$
				\item Conditional Probability and Bayles Theorem\\
				1. $P(A|B) = \frac{P(A\cup B)}{P(B)}$\\
				2. $P(A|B) = \frac{P(B|A)P(A)}{P(B)} = \frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|A')P(A')} $
				\item PDF and CDF\\
				PDF: probability distribution function $\Rightarrow P_X(x)$ \\
				CDF: cummulative distribution function $\Rightarrow F_X(x) = \phi_X(x) = N_X(x) = P(X < x)$
				\item Expectation\\
				1. $E(X) = \sum xP_X(x)$ or $\int_{-\infty}^{\infty}xP_X(x)$\\
				2. $E(c) = c$\\
				3. $E(aX) = aE(X)$\\
				4. $E(X+Y) = E(X) + E(Y)$\\
				5. $E(X) = M_X'(0)$
				\item Variance and Standard Deviation\\
				1. $VAR(X) = E(X^2)-E^2(X)$ = $\sum[(x-E(X)]P_X(x) = E[(x-\mu)^2] = \sigma^2$\\
				2. $VAR(c) = 0$\\
				3. $VAR(aX) = a^2VAR(x)$\\
				4. $VAR(X\pm Y) = VAR(X) + VAR(Y) \pm 2COV(x,y)$, $COV(x,y) = E(XY)-E(X)E(Y)$
				\item z-score\\
				$Z = \frac{x-\mu}{\sigma}$, measures the distance of x from expected value in standard units.
			\end{enumerate}
		
			\item Common Discrete Distributions
			\begin{enumerate}
				\item Binomial Distribution\\
				DEF: n time Bernoulli trials combined. probability of success and fail is (p, 1-p). Probability of success remains the same through the trails. X is the r.v of success times.\\
				Note as $X \sim B(n,p)$\\
				$$P_X(x) = {n \choose x} p^x (1-p)^{n-x}$$
				$$M_X(t) = (1-p+pe^t)^n$$
				$$E(X) = np, \ VAR(X) = np(1-p)$$
				\item Hyper Geometric Distribution\\
				DEF: A sample of size n taken from a finite population of size N. The population has a subgroup of size $r \ge n$ that is of interest. x is the number of members of the subgroup taken.\\
				Note as $X \sim H(N,n,r)$\\
				$$P_X(x) = \frac{{r \choose x}{N-r \choose n-x}}{{N \choose n}}$$
				$$E(X) = n\frac{r}{N}, \ VAR(X) = n\frac{r}{N}(1-\frac{r}{N})(\frac{N-n}{N-1})$$
				When $x \rightarrow \infty$, H(N,n,r) $\rightarrow$ B(n,$\frac{r}{N}$). H samples without replacement while B samples with replacement.
				\item Poisson Process\\
				DEF: model the number of random occurance of some phenomenon in specific unit of space or time.\\
				$$P_X(x) = \frac{e^{-\lambda}\lambda^x}{x!}$$
				$\lambda$: average arrival in given time or space\\
				$$E(X) = \lambda, VAR(X) = \lambda$$
				Poisson simulates Binomial when n is large. usually when $\lambda = np < 10$, we use poisson as an approximation of Binomial.
				\item Geometric Distribution\\
				DEF: number of trials to get the first success in a sequence of Bernoulli trials where p is the success probability.
				\begin{enumerate}
					\item X is the r.v of number of total trials (x includes the first success)\\
					$$P_X(x) = (1-p)^{x-1}p, x = 1,2,3 \dots$$
					$$E(X) = 1/p, VAR(X) = \frac{1-p}{p^2}$$
					\item X is the r.v of number of failed trials (x excludes the first success)\\
					$$ P_X(x) = (1-p)^xp, x = 0,1,2,\dots $$
					$$M_X(t) = \frac{p}{1-e^t(1-p)}$$
					$$E(X) = \frac{1-p}{p}, \ VAR(X) = \frac{1-p}{p^2}$$
				\end{enumerate}
				\item Negative Binomial Distribution\\
					DEF: X is the r.v of number of trials need to observe the $r^{th}$ success in a sequence of Bernoulli trails where p is the success probability.\\
					$$P_X(x) = {x-1 \choose r-1}p^r(1-p)^{x-r}, x = r,r+1,r+2 \dots$$
					$$M_X(t) = (\frac{p}{1-e^t(1-p)})^r$$
					$$E(X) = \frac{r}{p}, VAR(X) = \frac{r(1-p)}{p^2}$$
					Alternatively, X is the r.v of failures before the $r^{th}$ success:\\
					$$P_X(x) = {x+r-1 \choose r-1}p^r(1-p)^x, x = 0,1,2 \dots$$
					$$M_X(t) = (\frac{1-p}{1-pe^t})^r$$
					$$E(X) = \frac{r(1-p)}{p}, VAR(X) = \frac{r(1-p)}{p^2}$$
			\end{enumerate}
			\item Chebychev's Theorem\\
			$$P(\mu-k\sigma \le x \le \mu+k\sigma) \ge 1-\frac{1}{k^2}$$
			\item Continuous Random Variable
			\begin{enumerate}
				\item Basic Properties\\
				1. $pdf: f_X(x) \ge 0$\\
				2. $cdf: F_X(x) = \int_{-\infty}^{\infty} f_X(x)dx$\\
				3. $\int_{-\infty}^{\infty}f_X(x)dx = 1$\\
				4. $f_X(x) = \frac{d}{dx}F_X(x)$
				\item Mean, Medium and Variance\\
				mean: $\mu = E(x) = \int_{-\infty}^{\infty}xf_X(x)dx$\\
				medium m: solve function $\int_{-\infty}^{m} f_X(x)dx = \frac{1}{2}$\\
				Variance: \\
				$VAR(X) = E(X^2) - E^2(X) = \int_{-\infty}^{\infty}[x-E(X)]f_X(x)dx$
			\end{enumerate}
			\item Common Continuous Distributions
			\begin{enumerate}
				\item Uniform(rectangle) Distribution\\
				$$f_X(x) = \frac{1}{b-a}, a \le x \le b$$
				$$F_X(x) = \frac{x-a}{b-a}, a\le x \le b$$
				$$E(X) = \frac{a+b}{2}, VAR(X) = \frac{(b-a)^2}{12}$$
				\item Exponential Distribution\\
				$$f_X(x) = \frac{1}{\theta}e^{-\frac{x}{\theta}}, x \ge 0$$
				$$F_X(x) = 1- e^{-\frac{x}{\theta}}, x \ge 0$$
				$$M_X(t) = \frac{1}{1-\theta t}$$
				$$E(X) = \theta, VAR(X) = \theta^2$$
				Note as $X \sim exp(\theta)$\\
				In terms of $\lambda$: $\lambda = \frac{1}{\theta}$ 
				\item Gamma Distribution\\
				$$f_X(x)= \frac{1}{\Gamma(n)\theta^n}x^{n-1}e^{-\frac{x}{\theta}}, x \ge 0$$
				$$M_X(t) = \frac{1}{(1-\theta t)^n}$$
				$$E(X) = n\theta, VAR(X) = n\theta^2$$
				Note as $X \sim \Gamma(n,\theta)$\\
				Exponential dist. is a special case of Gamma dist where n = 1. Gamma Distribution can be viewed as a sum of Exponential dists.\\
				$X \sim \Gamma(n,\theta) \Leftrightarrow X = \sum_{i = 1}^{n} X_i, X_i \sim exp(\theta)$\\
				In terms of $\alpha, \beta$: ($\alpha = n, \beta = \frac{1}{\theta}$)
				$$f_X(x)= \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}, x \ge 0$$
				$$M_X(t) = \frac{1}{(1-\frac{t}{\beta})^\alpha}$$
				$$E(X) = \frac{\alpha}{\beta}, VAR(X) = \frac{\alpha}{\beta^2}$$
				\item Normal Distribution\\
				Standard normal distribution: \\
				X $\sim$ N(0,1), $\mu = 0,\sigma^2 = 1$\\
				Normal distribution:
				$$f_X(x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}, \ x \in (-\infty, \infty)$$
				$$M_X(t) = e^{\mu t + \frac{1}{2}\sigma^2 t^2}$$
				$$E(x) = \mu, \ VAR(X) = \sigma^2$$
				\item Log-normal Distribution:\\
				$Y = e^X, X \sim N(\mu, \sigma^2)$
				$$f_Y(y) = \frac{1}{\sigma y \sqrt{2 \pi}}e^{-\frac{1}{2}(\frac{lny - \mu}{\sigma})^2}, y \ge 0$$
				$$E(Y) = e^{\mu+\frac{\sigma^2}{2}}, VAR(Y) = e^{2\mu + \sigma^2}(e^{\sigma^2} - 1)$$
				\item Pareto Distribution:\\
				$$f_X(x) = \frac{k}{\beta}(\frac{\beta}{x})^{k+1}$$
				$$F_X(x) = 1 - (\frac{\beta}{x})^k, k > 2, x \ge \beta >0$$
				$$E(X) = \frac{k\beta^2}{k-1}, VAR(X) = \frac{k\beta^2}{k-2} - (\frac{k\beta}{k-1})^2$$
				Other notation: $\alpha = k, \beta = \beta$
				\item Weibull Distribution:\\
				$$f_X(x) = k\lambda x^{k-1}e^{-\lambda kx^k}$$
				$$F_X(x) = 1 - e^{-\lambda kx^k}$$
				$$E(X) = \frac{\Gamma(1+\frac{1}{k})}{\lambda^{\frac{1}{k}}}, VAR(X) = \frac{1}{\lambda^{\frac{2}{k}}}[\Gamma(1+\frac{2}{k}) - \Gamma^2(1+\frac{1}{k})]$$
				Other notation:$k = \alpha, \lambda = \beta$\\
				Note: $\Gamma(\frac{1}{2}) = \sqrt{\pi}$\\
				Proof:\\
				$1 - F_Z(0) = \frac{1}{2}$\\
				$\Rightarrow \frac{1}{\sqrt{2\pi}}\int_{0}^{\infty}e^{-\frac{1}{2}z^2}dz = \frac{1}{2}$\\
				Let $u = \frac{1}{2}z^2, du = zdz, z = \sqrt{2u}$\\
				$\Rightarrow \frac{1}{\sqrt{2\pi}}\int_{0}^{\infty}e^{-u}\frac{1}{\sqrt{2u}}du = \frac{1}{2}$\\
				$\Rightarrow \int_{0}^{\infty}e^{-u}u^{-\frac{1}{2}} = \sqrt{\pi}$\\
				$\Gamma(\frac{1}{2}) = \sqrt{\pi}$
				\item Beta Distribution
				$$f_X(x) = \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}, x \in (0,1), a>0, b>0$$
				$$E(X) = \frac{a}{a+b}, VAR(X) = \frac{ab}{(a+b)^2(a+b+1)}$$
				\item Chi-Square $\chi^2$ Distribution
				$$f_X(x) = \frac{1}{2^{\frac{k}{2}}\Gamma(\frac{k}{2})}x^{\frac{k}{2}-1}e^{-\frac{x}{2}}, x > 0$$
				$$E(X) = k, VAR(X) = 2k$$
				$$M_X(t) = \frac{1}{(1-2t)^{\frac{k}{2}}}$$
				This is a special case of Gamma Dist. of $\alpha = \frac{k}{2}, \ \beta = 2$\\
				$f_{Z^2}(z) = \frac{1}{\Gamma(\frac{1}{2})2^{\frac{1}{2}}}e^{\frac{y}{2}}y^{\frac{1}{2}-1}$(gamma dist of $\alpha = \frac{1}{2}, \beta = 2$)
				$\chi^2 = \sum_{i = 1}^{k}Z_i^2$(sum of k squared standard normal)
				Theorem:\\
				If $X_1, X_2 \dots, X_k$ is a random $\textbf{sample}$ from $N(\mu, \sigma^2)$\\
				1. $Z = \sum_{i = 1}^{k}(\frac{x_i-\mu}{\sigma})^2 \sim \chi^2(k)$ (mean)\\
				2. $Z = \sum_{i = 1}^{k}(\frac{x_i-\bar x}{\sigma})^2 \sim \chi^2(k-1)$ (sample mean)\\
			\end{enumerate}
			\item Central Limit Theorem\\
			Let $\{ X_1, X_2, \dots, X_n \}$ be the independent random variable all of which have the same distribution and mean $\mu$ and standard deviation $\sigma$. If n is large $n \ge 30$, then 
			$$ S = X_1 + X_2 + \dots + X_n$$
			will be approximately normal with mean $n\mu$ and variance $n\sigma^2$. $(S\sim N(n\mu, n\sigma^2))$\\
			$\textbf{OR}$\\
			$$S' = \frac{X_1 + X_2 + \dots + X_n}{n}$$
			will be approximately normal with mean $\mu$, variance $\frac{\sigma^2}{n}$.$(S\sim N(\mu, \frac{\sigma^2}{n}))$\\
			\item Finding CDF for $Y = g(X)$\\
			1. $g(x)$ is strictly increasing on the sample space for X\\
			Let $h(y)$ be the inverse function of $g(x)$. $h(x)$ is also strictly increasing.
			\begin{align*}
				F_Y(y) &= P(Y\le y)\\
				&= P(Y\le y)\\
				&= P(g(X) \le y)\\
				&= P[X \le h(y)]\\
				&= F_X(h(y))
			\end{align*}
			Find the density function by differentiating
			\begin{align*}
				f_Y(y) &= \frac{d}{dy}F_X(h(y))\\
				&= \frac{d}{dx}F_X(h(y))\frac{dy}{dx}\\
				&= \frac{d}{dx}F_X(h(y))h'(y)
			\end{align*}
			2. $g(x)$ is strictly decreasing on the sample space for X\\
			Let $h(y)$ be the inverse function of $g(x)$. $h(x)$ is also strictly decreasing.\\
			\begin{align*}
				F_Y(y) &= P(Y\le y)\\
				&= P(Y\le y)\\
				&= P(g(X) \le y)\\
				&= P[X \ge h(y)]\\
				&= 1 - F_X(h(y))
			\end{align*}
			Find the density function by differentiating
			\begin{align*}
				f_Y(y) &= \frac{d}{dy}[1-F_X(h(y))]\\
				&= -\frac{d}{dx}F_X(h(y))\frac{dy}{dx}\\
				&= -\frac{d}{dx}F_X(h(y))h'(y)
			\end{align*}
			IN ALL:\\
			Density function for $Y = g(X)$\\
			Let g(x) be strictly decreasing or increasing on the domain consisting of the sample space. Then:
			$$f_Y(y) = \frac{d}{dx}F_X(h(y)) \cdot |h'(y)|$$
			\item Multivariable Distributions
			\begin{enumerate}
				\item Marginal Distribution
				$$p_X(x)=\sum_{y}p(x,y), \ p_Y(y)=\sum_{x}p(x,y) $$
				$$f_X(x) = \int_{-\infty}^{\infty}p(x,y)dy, \ f_Y(y) = \int_{-\infty}^{\infty}p(x,y)dx$$
				\item Conditional Distribution
				$$P(X = x|Y = y) = \frac{p(x,y)}{p_Y(y)}, \ P(Y = y|X = x) = \frac{p(x,y)}{p_X(x)}$$
				$$f(x|Y=y)=\frac{f(x,y)}{f_Y(y)}, \ f(y|X=x)=\frac{f(x,y)}{f_X(x)}$$
				\item Conditional Expected Value
				$$E(y|X=x) = \sum_{y}y\cdot p(y|x), \ E(x|Y=y) = \sum_{x}x\cdot p(x|y)$$
				$$E(y|X=x) = \int_{-\infty}^{\infty}y\cdot f(y|x)dy, \ E(x|Y=y) = \int_{-\infty}^{\infty}x\cdot f(x|y)dx$$
				\item Independence
				$$p(x,y) = p_X(x) \cdot p_Y(y)$$
				$$f(x|y) = f_X(x), \ f(y|x) = f_Y(y)$$
				$$E(X+Y) = E(X)+E(Y)$$
				$$COV(X,Y) = 0$$
				$$VAR(X \pm Y) = VAR(X) + VAR(Y)$$
				\item Covariance
				$$COV(x,y) = E[(X-\mu_X)(Y-\mu_Y)] = E(XY) - E(X)E(Y)$$
				Properties:
				\begin{enumerate}
					\item $COV(X,Y) = COV(Y,X)$
					\item $COV(X,X) = VAR(X)$
					\item $COV(X,K) = 0, K$ is constant
					\item $COV(aX,bY) = ab\cdot COV(X,Y)$
					\item $COV(X,Y+Z) = COV(X,Y) + COV(X,Z)$
					\item If $X, Y$ are independent, $COV(X,Y) = 0$
				\end{enumerate}
				\item Correlation Coefficient $\rho$\\
				$$\rho = \rho_{X,Y}= \frac{COV(X,Y)}{\sigma_X \cdot \sigma_Y}, \in [-1,1]$$
				$|\rho_{X,Y}|$ close to 1 : more related\\
				$\rho > 0$ positively related, $\rho < 0$ negatively related
				\item Moment Generating Function of Joint Distribution
				$$M_{X,Y}(t_1, t_2) = E(e^{t_1X + t_2Y})$$
				$$E(X)=\frac{d}{dt_1}M_{X,Y}(t1,t2)|_{t_1=t_2=0}$$
				$$E(Y) = \frac{d}{dt_2}M_{X,Y}(t1,t2)|_{t_1=t_2=0}$$
				$$E(X^mY^n) = \frac{\partial^{m+n}}{\partial^mt_1\partial^nt_2}M_{X,Y}(t_1, t_2)|_{t_1=t_2=0}$$
				\item Multinomial Distribution\\
				Counting Partitions:\\
				The number of Partitions of $n$ objects into $k$ distinct groups of size $n_1, n_2 \dots n_k$ is given by:
				$${n \choose {n_1, n_2, \dots , n_k}} =  \frac{n!}{n_1!n_2! \dots n_k!}$$
				Suppose that a random experiment has $K$ mutually exclusive outcomes $E_1, E_2, \dots E_k$, with $P(E_i) = p_i , i \in [1,k]$. Suppose you repeat this experiment in $n$ independent trails. Then:\\
				$P(X_1 = E_1, \dots , X_k = E_k) = {n \choose {n_1, n_2, \dots , n_k}}p_1^{n_1}p_2^{n_2} \dots p_k^{n_k}$
			\end{enumerate}
		\end{enumerate}
	\newpage
	\end{multicols}
\end{document}